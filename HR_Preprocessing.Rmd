---
title: "HR Preprocessing"
author: "Aulia Dini Rafsanjani"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Goal

This code aims to : 

1. Check the count and percentage of ectopic beats based on threshold (< 0.3s and > 1.3s) VS the category of artifacts (category U and I) from the device. The statistics is computed for overall days and per-day statistics. 

2. Apply filtering and imputation to the heart rate data based on paper by :

Benchekroun M, Chevallier B, Istrate D, Zalc V, Lenne D. Preprocessing Methods for Ambulatory HRV Analysis Based on HRV Distribution, Variability and Characteristics (DVC). Sensors (Basel). 2022 Mar 3;22(5):1984. doi: 10.3390/s22051984. PMID: 35271128; PMCID: PMC8914897.

### Meeting on May 20th, 2024

1. Compute how many artifacts that are overalapped between paper and device. 

2. Compute correlation between the artifacts from paper and time-of-day. 

3. Re-check the filtering code. 

### Meeting on May 22nd, 2024

1. Compute correlation between the artifacts from paper and time-of-day (trend for each day). 

### Meeting on June 3rd, 2024

Create the additional condition of the filtering loop :

1. If the final RR > 1.3 s, delete it --> To handle the issue from Mary and Ben data. 

2. If the final RR is within the accepted range of 0.3s - 1.3s, leave it as it is. 

3. If the final RR < 0.3s, do left merge only. If the result of the left merge is <0.3s, do left merge again. If the result of left merge > 1.3 s, remove it. --> To handle the issue from Marie-Anne data. 

### Meeting on June 10th, 2024

Start coding for imputation

### Library

```{r}
library(data.table)
library(dplyr)
library(tidyr)
library(data.table)
library(dplyr)
library(ggplot2)
library(gridExtra)
```
### Answer the 1st goal

### Compute Count and Percentage of artifacts (Overall)

```{r}
# Define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

result_df1 <- data.frame()
result_df2 <- data.frame()

for (file in file_list) {
  
  # File path
  full_file_path <- file.path(directory, file)
  
  # Read the CSV file
  df <- fread(full_file_path, sep = "\t")
  
  # Change column name
  colnames(df)[4] <- "category"
  
  # convert RR into seconds
  df$RR_second = df$RR / 1000
  
  # Create table of category percentages
  table1 <- df %>%
  summarise(
    count_below_0.3 = sum(RR_second < 0.3),
    count_above_1.3 = sum(RR_second > 1.3),
    count_paper = count_below_0.3 + count_above_1.3,
    percent_below_0.3 = round((mean(RR_second < 0.3) * 100),2),
    percent_above_1.3 = round((mean(RR_second > 1.3) * 100),2),
    percent_paper = percent_below_0.3 + percent_above_1.3) %>%
    mutate(data_name = gsub("\\..*$", "", file))
  
  # Create table of RR summary (in seconds)
  table2 <- df %>%
  summarise(
    count_U = sum(category=="U"),
    count_I = sum(category=="|"),
    count_device = count_U + count_I,
    percent_U = round((mean(category=="U") * 100),2), 
    percent_I = round((mean(category=="|") * 100),2), 
    percent_device = percent_U + percent_I) %>%
    mutate(data_name = gsub("\\..*$", "", file))
  
  # Append the percentage table to the result data frame
  result_df1 <- bind_rows(result_df1, table1)
  result_df2 <- bind_rows(result_df2, table2)

}

# set the column name of df1
result_df1 <- result_df1[, c("data_name", "count_below_0.3", "count_above_1.3","count_paper", "percent_below_0.3" , "percent_above_1.3", "percent_paper")]
result_df1

# set the column name of df2
result_df2 <- result_df2[, c("data_name", "count_U", "count_I", "count_device", "percent_U", "percent_I", "percent_device")]
result_df2

# merge the subsetted data frames
df1_subset <- result_df1 %>% select(data_name, count_below_0.3, count_above_1.3, count_paper, percent_below_0.3 , percent_above_1.3, percent_paper)
df2_subset <- result_df2 %>% select(data_name, count_device, percent_device)

# print the final result
merged_df <- left_join(df1_subset, df2_subset, by = "data_name")
merged_df
```

Finding : 

- The proportion of artifacts from paper (threshold < 0.3s or  > 1.3s) and device (categorization of U and I) is different. There is not any clear pattern between the count / percentage from paper and device for overall proportion. 

### Compute Count and Percentage of artifacts (Per hours)

To compute the artifacts per hour, I compute total hours per person (variable name: max_hours) then I divide the count and percentage of artifacts with the max_hours. The result should be smaller than count and percentage of artifacts for overall hours. 

```{r}
# Define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

# Set the file list
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# Set the output
result_df1 <- data.frame()
result_df2 <- data.frame()

for (file in file_list) {
  
  # File path
  full_file_path <- file.path(directory, file)
  
  # Read the CSV file
  df <- fread(full_file_path, sep = "\t")
  
  # Change column name
  colnames(df)[4] <- "category"
  
  # convert RR into seconds
  df$RR_second = df$RR / 1000
  
  # compute hours per person
  max_hours = max(df$Time)/(1000*3600)
  
  # Create table of category percentages
  table1 <- df %>%
  summarise(
    count_below_0.3 = round(((sum(RR_second < 0.3))/max_hours),2),
    count_above_1.3 = round(((sum(RR_second > 1.3))/max_hours),2),
    count_paper = count_below_0.3 + count_above_1.3,
    percent_below_0.3 = round((mean(RR_second < 0.3) * 100)/max_hours,4),
    percent_above_1.3 = round((mean(RR_second > 1.3) * 100)/max_hours,4),
    percent_paper = percent_below_0.3 + percent_above_1.3) %>%
    mutate(data_name = gsub("\\..*$", "", file))
  
  # Create table of RR summary (in seconds)
  table2 <- df %>%
  summarise(
    count_U = round((sum(category=="U")/max_hours),2),
    count_I = round((sum(category=="|")/max_hours),2),
    count_device = count_U + count_I,
    percent_U = round((mean(category=="U") * 100)/max_hours,4), 
    percent_I = round((mean(category=="|") * 100)/max_hours,4), 
    percent_device = percent_U + percent_I) %>%
    mutate(data_name = gsub("\\..*$", "", file))
  
  # Append the percentage table to the result data frame
  result_df1 <- bind_rows(result_df1, table1)
  result_df2 <- bind_rows(result_df2, table2)

}

# Set the column name of result 1
result_df1 <- result_df1[, c("data_name", "count_below_0.3", "count_paper", "count_above_1.3", "percent_below_0.3" , "percent_above_1.3", "percent_paper")]
# Print the result
result_df1

# Set the column name of result 2
result_df2 <- result_df2[, c("data_name", "count_U", "count_I", "count_device", "percent_U", "percent_I", "percent_device")]
# Print the result
result_df2

# Merge the subsetted data frames
df1_subset <- result_df1 %>% select(data_name, count_below_0.3, count_above_1.3, count_paper, percent_below_0.3 , percent_above_1.3, percent_paper)
df2_subset <- result_df2 %>% select(data_name, count_device, percent_device)
merged_df <- left_join(df1_subset, df2_subset, by = "data_name")
# Print the result
merged_df
```

Findings:

- The proportion of artifacts from paper (threshold) and device (categorization of U and I) per hour is different. There is not any clear pattern between the count / percentage from paper and device for overall proportion. 

- This finding is consistent with the overall count and percentage result. 

### Follow up : Check how many artifacts based on thresholds that is overlapped with the artifacts from the device flags. 

Note :

Artifacts from paper is defined by threshold < 0.3 s or > 1.3 s. 

Artifacts from device is defined by category U and I. 

```{r}
# Define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

# call all datasets
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# define the results
result_df1 <- data.frame()

for (file in file_list) {
  
  # File path
  full_file_path <- file.path(directory, file)
  
  # Read the CSV file
  df <- fread(full_file_path, sep = "\t")
  
  # Change column name
  colnames(df)[4] <- "category"
  
  # convert RR into seconds
  df$RR_second = df$RR / 1000
  
  # create dummy to define artifacts criteria
  df$dummy_thres <- ifelse(df$RR_second < 0.3 | df$RR_second > 1.3, 1, 0) # based on threshold in paper
  df$dummy_device <- ifelse(df$category=="U" | df$category=="|", 1, 0) # based on device
  df$dummy_both <- ifelse(df$dummy_device==1 & df$dummy_thres==1, 1, 0) # categorized as artifacts in both paper and device
  
  # compute count and percentages
  table1 <- df %>%
  summarise(count_paper = sum(dummy_thres==1),
            count_device = sum(dummy_device==1), 
            count_overlap = sum(dummy_both==1),
            percent_paper = round((mean(dummy_thres) * 100),2),
            percent_device = round((mean(dummy_device) * 100),2), 
            percent_overlap = round((mean(dummy_both) * 100),2)
            ) %>%
    mutate(data_name = gsub("\\..*$", "", file))
  
  # Append the percentage table to the result data frame
  result_df1 <- bind_rows(result_df1, table1)
}

# print the result
result_df1 <- result_df1[, c("data_name", "count_paper", "count_device", "count_overlap", "percent_paper" , "percent_device", "percent_overlap")]
result_df1
```

Findings:

- There are some overlapped artifacts between the paper and the device. 

- The overlapped percentage is less than 7 percent. 

### Follow up: Compute correlation of time-of-day and artifacts from paper. 

### 1. Create helper function to convert the Time (in milliseconds) to standard time format in HH:MM:SS.

This is the helper function to convert the Time (in milliseconds) to standard time format in HH:MM:SS. The input of this helper function is the data itself and the initial time showed in the first line of the data. The 2nd helper function shows how to extract the initial time and then input it to this helper function. 

```{r}
# library
library(data.table)
library(lubridate)

process_time <- function(df, initial_time) {
  
  # set the initial time format
  start_time <- as.POSIXct(initial_time, format = "%H:%M:%S")
  
  # convert the Time from milliseconds to seconds
  df$time_seconds <- as.numeric(df$Time) / 1000
  
  # convert the RR from milliseconds to seconds
  df$RR_second <- df$RR / 1000
  
  # create the clock time to convert the time in seconds to the clock format
  df$clock_time <- start_time + df$time_seconds
  
  # extract the time only and exclude the date information
  df$time_only <- format(df$clock_time, format = "%H:%M:%S")
  
  # categorize the time into 4 hour time-of-day 
  df$cat_time <- fifelse(df$time_only >= "00:00:00" & df$time_only < "04:00:00", 1,
                            fifelse(df$time_only >= "04:00:00" & df$time_only < "08:00:00", 2,
                                    fifelse(df$time_only >= "08:00:00" & df$time_only < "12:00:00", 3,
                                            fifelse(df$time_only >= "12:00:00" & df$time_only < "16:00:00", 4,
                                                    fifelse(df$time_only >= "16:00:00" & df$time_only < "20:00:00", 5,
                                                            fifelse(df$time_only >= "20:00:00" & df$time_only < "24:00:00", 6, NA))))))
  
  # create dummy for artifacts coded as 1 if RR < 0.3 s or RR > 1.3 s
  df$dummy_thres <- ifelse(df$RR_second < 0.3 | df$RR_second > 1.3, 1, 0)
  
  return(df)
}

```

### 2. Create helper function to extract the start time for each dataset. 

The result of this helper function is start time for each participant with format of HH:MM:SS. I set the time format into 24 hour so that the conversion to the time-of-day is easier.

```{r}
library(data.table)

directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# set result as list
start_times_list <- list()

for (i in seq_along(file_list)) {
  
    # input the file list
    file <- file_list[i]
    
    # file path
    full_file_path <- file.path(directory, file)

    # Read the first line to get the recording start time
    first_line <- tryCatch({
      readLines(full_file_path, n = 1)
    }, error = function(e) {
      NA
    })

    if (!is.na(first_line)) {
      
      # extract the recording start time 
      time_pattern <- "Recording start time:\\s*(\\d{1,2}:\\d{2}:\\d{2}\\s*[AP]M)"
      start_time <- sub(time_pattern, "\\1", first_line)

      # reformat the start time into 24 hr
      start_time_24h <- format(strptime(start_time, "%I:%M:%S %p"), "%H:%M:%S")
      
      # append the start time to the list
      start_times_list[[i]] <- start_time_24h
      
    } else {
      
      # Handle case where first line could not be read
      start_times_list[[i]] <- NA
    }
}

# Print the list of start times
print(start_times_list)
```

### 3rd. Create helper function to compute count_artifacts and percent_artifacts in each time-of-day

```{r}
# library
library(tidyr)
library(dplyr)

# define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

# call all data set
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# initial times
initial_times <- start_times_list

# define the result data frame
result_df1 <- data.frame()
result_df2 <- data.frame()

for (i in seq_along(file_list)) {
  
  # file path
  full_file_path <- file.path(directory, file_list[i])
  
  # read the CSV file
  df <- fread(full_file_path, sep = "\t")
  
  # change column name
  colnames(df)[4] <- "category"
  
  # process the data, this is the result from the first and second helper function
  df <- process_time(df, initial_times[[i]])
  
  # compute count and percentages
  table1 <- df %>%
            group_by(cat_time) %>%
            summarise(count_artifacts = sum(dummy_thres)) %>%
    mutate(data_name = gsub("\\..*$", "", file_list[i]))
  
    # compute count and percentages
  table2 <- df %>%
            group_by(cat_time) %>%
            summarise(percent_artifacts = round((sum(dummy_thres) / sum(df$dummy_thres) * 100),2)) %>%
    mutate(data_name = gsub("\\..*$", "", file_list[i]))
  
  # append the percentage table to the result data frame
  result_df1 <- bind_rows(result_df1, table1)
  result_df2 <- bind_rows(result_df2, table2)
}

# reshape result to wide format
df1_wide <- result_df1 %>%
  pivot_wider(names_from = cat_time, values_from = count_artifacts, values_fill = list(count_artifacts = 0))
colnames(df1_wide)[2:7] <- c("cat_time_1", "cat_time_2", "cat_time_3", "cat_time_4", "cat_time_5", "cat_time_6")

# reshape result to wide format
df2_wide <- result_df2 %>%
  pivot_wider(names_from = cat_time, values_from = percent_artifacts, values_fill = list(percent_artifacts = 0))
colnames(df2_wide)[2:7] <- c("cat_time_1", "cat_time_2", "cat_time_3", "cat_time_4", "cat_time_5", "cat_time_6")

# print the result
print(df1_wide)
print(df2_wide)
```

### 4th. Create plot to visualize count_artifacts and percent artifacts

```{r}
# library
library(tidyverse)

# plot for count_artifacts
# convert data from wide format to long format
df1_long <- df1_wide %>%
  pivot_longer(
    cols = starts_with("cat_time"),
    names_to = "cat_time",
    values_to = "count_artifacts"
  ) %>%
  mutate(cat_time = as.numeric(str_replace(cat_time, "cat_time_", "")))

# create plot
ggplot(df1_long, aes(x = cat_time, y = count_artifacts, fill = data_name)) +
  geom_col() +
  facet_wrap(~ data_name, scales = "free_y") +
  labs(title = "Count of Artifacts by Categorized Time for Each Dataset",
       x = "Categorized Time-of-Day",
       y = "Count of Artifacts") +
  scale_x_continuous(breaks = 1:6, labels = 1:6) +
  theme_minimal() +
  theme(legend.position = "none")

# plot for percent_artifacts
# convert data from wide format to long format
df2_long <- df2_wide %>%
  pivot_longer(
    cols = starts_with("cat_time"),
    names_to = "cat_time",
    values_to = "percent_artifacts"
  ) %>%
  mutate(cat_time = as.numeric(str_replace(cat_time, "cat_time_", "")))

# create plot
ggplot(df2_long, aes(x = cat_time, y = percent_artifacts, fill = data_name)) +
  geom_col() +
  facet_wrap(~ data_name, scales = "free_y") +
  labs(title = "Percent of Artifacts by Categorized Time for Each Dataset",
       x = "Categorized Time-of-Day",
       y = "Percent of Artifacts") +
  scale_x_continuous(breaks = 1:6, labels = 1:6) +
  theme_minimal() +
  theme(legend.position = "none")
```
Note :

- Y-axis shows the count and percentage of artifacts, X-axis shows the time-of-day with category 1 for (00:00:00-04:00:00), category 2 for (04:00:00-08:00:00), category 3 for (00:08:00-12:00:00), category 4 for (12:00:00-16:00:00), category 5 for (16:00:00-20:00:00), category 6 for (20:00:00-24:00:00). 

- In this pilot study, we have 9 participants. Participant 19003 and 86760 are the real participant and they wear the device for seven days. 

- The other participant is the lab's staffs, so they didn't wear the device for the whole week. 

Findings for count_artifacts:

- The distribution of count_artifacts for participant is high in certain times. The high peak is different for each participant. 

Findings for percent_artifacts:

- The distribution of percent_artifacts for participant is high in certain times. The high peak is different for each participant. 

- This result is consistent with the count_artifacts.

### Follow up : Create trend of artifacts by day and time-of-day

```{r}
# library
library(data.table)
library(lubridate)

process_date_time <- function(df, initial_time) {

directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file_list <- c("test867601_ADVHRV_2024-05-07.txt")

# File path
full_file_path <- file.path(directory, file_list)
  
# Read the CSV file
df <- fread(full_file_path, sep = "\t")
  
# Change column name
colnames(df)[4] <- "category" 

initial_time <- "2024-02-02 00:44:49"
  
# Set the initial time format
start_time <- as.POSIXct(initial_time, format = "%Y-%m-%d %H:%M:%S")

# Convert the Time from milliseconds to seconds
df$time_seconds <- as.numeric(df$Time) / 1000

# Convert the RR from milliseconds to seconds
df$RR_second <- df$RR / 1000

# Create the clock time to convert the time in seconds to the clock format
df$clock_time <- start_time + df$time_seconds

# Extract the date and time information
df$date <- as.Date(df$clock_time)
df$time_only <- format(df$clock_time, format = "%H:%M:%S")

# Define the function to categorize the time into 6-hour intervals
categorize_time <- function(time) {
  if (time >= "00:00:00" & time < "04:00:00") {
    return(1)
  } else if (time >= "04:00:00" & time < "08:00:00") {
    return(2)
  } else if (time >= "08:00:00" & time < "12:00:00") {
    return(3)
  } else if (time >= "12:00:00" & time < "16:00:00") {
    return(4)
  } else if (time >= "16:00:00" & time < "20:00:00") {
    return(5)
  } else if (time >= "20:00:00" & time < "24:00:00") {
    return(6)
  } else {
    return(NA)
  }
}

# Apply the categorization function to the time_only column
df$cat_time <- sapply(df$time_only, categorize_time)

# Create a combined date and time category column
df$cat_date_time <- paste(df$date, "time", df$cat_time, sep = "_")

# create dummy for artifacts coded as 1 if RR < 0.3 s or RR > 1.3 s
  df$dummy_thres <- ifelse(df$RR_second < 0.3 | df$RR_second > 1.3, 1, 0)
  
  return(df)
  
}
```

```{r}
# library
library(data.table)
library(lubridate)

process_date_time <- function(df, initial_time) {

# Set the initial time format
start_time <- as.POSIXct(initial_time, format = "%Y-%m-%d %H:%M:%S")

# Convert the Time from milliseconds to seconds
df$time_seconds <- as.numeric(df$Time) / 1000

# Convert the RR from milliseconds to seconds
df$RR_second <- df$RR / 1000

# Create the clock time to convert the time in seconds to the clock format
df$clock_time <- start_time + df$time_seconds

# Extract the date and time information
df$date <- as.Date(df$clock_time)
df$time_only <- format(df$clock_time, format = "%H:%M:%S")

# Define the function to categorize the time into 6-hour intervals
categorize_time <- function(time) {
  if (time >= "00:00:00" & time < "04:00:00") {
    return(1)
  } else if (time >= "04:00:00" & time < "08:00:00") {
    return(2)
  } else if (time >= "08:00:00" & time < "12:00:00") {
    return(3)
  } else if (time >= "12:00:00" & time < "16:00:00") {
    return(4)
  } else if (time >= "16:00:00" & time < "20:00:00") {
    return(5)
  } else if (time >= "20:00:00" & time < "24:00:00") {
    return(6)
  } else {
    return(NA)
  }
}

# Apply the categorization function to the time_only column
df$cat_time <- sapply(df$time_only, categorize_time)

# Create a combined date and time category column
df$cat_date_time <- paste(df$date, "time", df$cat_time, sep = "_")

# create dummy for artifacts coded as 1 if RR < 0.3 s or RR > 1.3 s
  df$dummy_thres <- ifelse(df$RR_second < 0.3 | df$RR_second > 1.3, 1, 0)
  
  return(df)
  
}
```

```{r}
# library
library(tidyr)
library(dplyr)

# define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

# call all data set
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# initial times
# Assuming initial_time is in "YYYY-MM-DD HH:MM:SS" format
initial_times <- list(
  "2024-02-02 00:44:49",
  "2024-01-12 17:13:09",
  "2024-01-26 08:55:39",
  "2024-01-12 21:06:23",
  "2024-01-26 10:50:17",
  "2024-01-12 14:38:09",
  "2024-01-15 14:54:15",
  "2024-04-16 16:24:33",
  "2024-04-12 15:58:52"
)

# define the result data frame
result_df1 <- data.frame()
result_df2 <- data.frame()

for (i in seq_along(file_list)) {
  
  # file path
  full_file_path <- file.path(directory, file_list[i])
  
  # read the CSV file
  df <- fread(full_file_path, sep = "\t")
  
  # change column name
  colnames(df)[4] <- "category"
  
  # process the data, this is the result from the first and second helper function
  df <- process_date_time(df, initial_times[[i]])
  
  # compute count and percentages
  table1 <- df %>%
            group_by(cat_date_time) %>%
            summarise(count_artifacts = sum(dummy_thres)) %>%
    mutate(data_name = gsub("\\..*$", "", file_list[i]))
  
    # compute count and percentages
  table2 <- df %>%
            group_by(cat_date_time) %>%
            summarise(percent_artifacts = round((sum(dummy_thres) / sum(df$dummy_thres) * 100),2)) %>%
    mutate(data_name = gsub("\\..*$", "", file_list[i]))
  
  # append the percentage table to the result data frame
  result_df1 <- bind_rows(result_df1, table1)
  result_df2 <- bind_rows(result_df2, table2)
}

result_df1
result_df2

# reshape result to wide format
#df1_wide <- result_df1 %>%
  #pivot_wider(names_from = cat_date_time, values_from = count_artifacts, values_fill = list(count_artifacts = 0))
#colnames(df1_wide)[2:7] <- c("cat_time_1", "cat_time_2", "cat_time_3", "cat_time_4", "cat_time_5", "cat_time_6")

# reshape result to wide format
#df2_wide <- result_df2 %>%
#  pivot_wider(names_from = cat_date_time, values_from = percent_artifacts, values_fill = list(percent_artifacts = 0))
#colnames(df2_wide)[2:7] <- c("cat_time_1", "cat_time_2", "cat_time_3", "cat_time_4", "cat_time_5", "cat_time_6")

# print the result
#print(df1_wide)
#print(df2_wide)
```
### Create time-of-day for every person

```{r}
library(ggplot2)
library(gridExtra)

df <- result_df2

# Assuming your data frame is named `df`
# Get the unique dataset names
dataset_names <- unique(df$data_name)

# Create a list to store the plots
plot_list <- list()

# Loop through each dataset name and create a plot
for (dataset in dataset_names) {
  
  # Subset the data for the current dataset
  subset_df <- df[df$data_name == dataset, ]
  
  # Create the plot
  p <- ggplot(subset_df, aes(x = cat_date_time, y = percent_artifacts, fill = data_name)) +
    geom_col() +
    labs(title = paste("Percent of Artifacts:", dataset),
         x = "Categorized Time-of-Day",
         y = "Percent of Artifacts") +
    theme_minimal() +
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Add the plot to the list
  plot_list[[dataset]] <- p
}

plot_list
```


### Filtering

### Filtering 2 (Revised) --> Presented in the meeting on Friday May 24th, 2024. 

```{r}
library(dplyr)

filtering <- function(df) {

# convert RR to seconds
df <- df %>%
      mutate (RR=RR/1000)

# helper function to compute E_10
compute_E_10 <- function(df,i) {
  if(i+10 > nrow(df) || i-1 < 1) return (NA)
  sum(abs((df$RR[i:(i+10)] - df$RR[(i-1):(i+9)]) / df$RR[(i-1):(i+9)])) /10
} 

# process the data
# for RRi < 0.3s compute RR_R, E_r, E_l, E_totr, E_10
for (i in 1:(nrow(df)-1)) {
  
  if (!is.na(df$RR[i]) && df$RR[i] < 0.3){
    if (i+1 > nrow(df)) break
      RR_r <- df$RR[i] + df$RR[i+1]
      ER_r <- abs((RR_r - df$RR[i]) / df$RR[i])
      ER_l <- abs((RR_r - df$RR[i-1]) / df$RR[i-1])
      E_tot_r <- if (!is.na(ER_r) && !is.na(ER_l)) ER_r + ER_l else NA
      E_10 <- compute_E_10(df,i)
   
    #  if RR_r < 1.3s and ER_l <= E_10 and ER_r <= E_10 then 
      # do right merge, replace RR_i+1 by RR_r and delete RR_i and its timestamp
    if (!is.na(RR_r) && !is.na(ER_r) && !is.na(ER_l) && !is.na(E_10) && RR_r < 1.3 && ER_l <= E_10 && ER_r <= E_10){
      df$RR[i+1] <- RR_r
      df <- df[-i,]
      
      # else compute RR_l = RR_i + RR_i-1 and EL_r (equation 6), EL_l (equation 7) and Etot_l (equation 5)
    } else { 
      if (i-1 < 1) break
      RR_l <- df$RR[i] + df$RR[i-1]
      EL_r <- abs((RR_l - df$RR[i]) / df$RR[i])
      EL_l <- abs((RR_l - df$RR[i-1]) / df$RR[i-1])
      E_tot_l <- if (!is.na(EL_r) && !is.na(EL_l)) EL_r + EL_l else NA
      E_10_prev <- compute_E_10(df, i-1)
      
      # if RR_l < 1.3 s and EL_l <= E10 and EL_r <= E_10 then 
      # left merge, replace R_i by RR_l and delete RR_i-1 and its timestamp
    if (!is.na(RR_l) && !is.na(EL_l) && !is.na(EL_r) && !is.na(E_10_prev) && RR_l < 1.3 && EL_l <= E_10_prev && EL_r <= E_10_prev) {
      df$RR[i-1] <- RR_l
      df <- df[-i,]
      
      # else if RR_r > 1.3s and RR_l > 1.3 s then
      # delete both RR_i and RR_i+1
    } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l > 1.3) {
      if (i+1 <= nrow(df)) {
        df <- df[-c(i,i+1),]
      } else {
        df <- df[-i,] }
      
      # else if RR_r < 1.3 s and RR_l > 1.3 s then 
      # Replace RR_i+1 and RR_r and delete RR_i and its timestamp
      } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r < 1.3 && RR_l > 1.3){
        df$RR[i+1] <- RR_r
        df <- df[-i,]
        
        # else if RR_r > 1.3 s and RR_l < 1.3 s then 
        # replace RR_i-1 by RR_l and delete RR_i and its timestamp
      } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l < 1.3){
       df$RR[i-1] <- RR_l
       df <- df[-i,]
       
       # else if both RR_l and RR_r < 1.3 s but errors Etot_r and Etot_l are higher than 0.4 then 
       # keep the one with smaller error
      } else if (!is.na(RR_l) && !is.na(RR_r) && !is.na(E_tot_r) && !is.na(E_tot_l) && RR_l < 1.3 && RR_r < 1.3 && E_tot_r > 0.4 && E_tot_l > 0.4){
        if (E_tot_r < E_tot_l){ # if right < left, use right 
         df$RR[i+1] <- RR_r
         df <- df[-i,]
        } else { # otherwise, use left
          df$RR[i-1] <- RR_l
          df <- df[-i,]
        }
      }
    }
  }
}

df <- df %>% mutate(RR = RR * 1000)

return(df)
}
```

### Test data

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("testben1_ADVHRV_2024-02-02_1244.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)
```

Findings based on meeting May 24, 2024: 

- The filtered dataset shows that the observation is within range of < 0.3 s. However, we still have 18999 observation that is > 1.3s. 

- We need to add more conditions : If RR > 1.3 s, remove the RR and its timestamps (based on paper flowchart page 5). 

### Filtering 3 (Revised) --> Presented in the meeting on June 3, 2024

- After discussion with Konstantinos, we found that all RR > 1.3 should be removed in the first place. 

- Therefore, I add one more condition which is for RR > 1.3 s, remove the RR and its timestamps. 

- In addition, I also change the `for` loop to `while` because some RR is missed if we used `for` function. 

```{r}
library(dplyr)

filtering <- function(df) {
  
  # Convert RR to seconds
  df <- df %>% mutate(RR = RR / 1000)
  
  # Helper function to compute E_10
  compute_E_10 <- function(df, i) {
    if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
    sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
  }
  
  # Process the data
  # set the starting point
  i <- 1
  while (i < nrow(df)) {
    
    # If RR > 1.3 s, then remove RR_i and its timestamp
    # this is the new condition
    if (!is.na(df$RR[i]) && df$RR[i] > 1.3) {
      df <- df[-i, ]
      next
    }
    
    # if RR < 0.3s, then do all the merging process
    if (!is.na(df$RR[i]) && df$RR[i] < 0.3) {
      if (i + 1 > nrow(df)) break
      RR_r <- df$RR[i] + df$RR[i + 1]
      ER_r <- abs((RR_r - df$RR[i]) / df$RR[i])
      ER_l <- abs((RR_r - df$RR[i - 1]) / df$RR[i - 1])
      E_tot_r <- if (!is.na(ER_r) && !is.na(ER_l)) ER_r + ER_l else NA
      E_10 <- compute_E_10(df, i)
      
      # If RR_r < 1.3s and ER_l <= E_10 and ER_r <= E_10 then 
      # do right merge, replace RR_i+1 by RR_r and delete RR_i and its timestamp
      if (!is.na(RR_r) && !is.na(ER_r) && !is.na(ER_l) && !is.na(E_10) && RR_r < 1.3 && ER_l <= E_10 && ER_r <= E_10) {
        df$RR[i + 1] <- RR_r
        df <- df[-i, ]
        next
      } else {
        if (i - 1 < 1) break
        RR_l <- df$RR[i] + df$RR[i - 1]
        EL_r <- abs((RR_l - df$RR[i]) / df$RR[i])
        EL_l <- abs((RR_l - df$RR[i - 1]) / df$RR[i - 1])
        E_tot_l <- if (!is.na(EL_r) && !is.na(EL_l)) EL_r + EL_l else NA
        E_10_prev <- compute_E_10(df, i - 1)
        
        # If RR_l < 1.3 s and EL_l <= E10 and EL_r <= E_10 then 
        # left merge, replace RR_i by RR_l and delete RR_i-1 and its timestamp
        if (!is.na(RR_l) && !is.na(EL_l) && !is.na(EL_r) && !is.na(E_10_prev) && RR_l < 1.3 && EL_l <= E_10_prev && EL_r <= E_10_prev) {
          df$RR[i - 1] <- RR_l
          df <- df[-i, ]
          next
        } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l > 1.3) {
          if (i + 1 <= nrow(df)) {
            df <- df[-c(i, i + 1), ]
          } else {
            df <- df[-i, ]
          }
          next
        } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r < 1.3 && RR_l > 1.3) {
          df$RR[i + 1] <- RR_r
          df <- df[-i, ]
          next
        } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l < 1.3) {
          df$RR[i - 1] <- RR_l
          df <- df[-i, ]
          next
        } else if (!is.na(RR_l) && !is.na(RR_r) && !is.na(E_tot_r) && !is.na(E_tot_l) && RR_l < 1.3 && RR_r < 1.3 && E_tot_r > 0.4 && E_tot_l > 0.4) {
          if (E_tot_r < E_tot_l) {
            df$RR[i + 1] <- RR_r
            df <- df[-i, ]
          } else {
            df$RR[i - 1] <- RR_l
            df <- df[-i, ]
          }
          next
        }
      }
    }
    # restate the iteration
    i <- i + 1
  }
  
  df <- df %>% mutate(RR = RR * 1000)
  
  return(df)
}

```

### Test data

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("testben1_ADVHRV_2024-02-02_1244.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)
```

```{r}
extract1 <- subset(data_filtered, RR>1300)
dim(extract1)
```


```{r}
View(extract1)
```

Findings :

- The data from Ben shows that there is one observation that is above the range (>1.3s). 

- After checking, the one observation refers to the last observation, which not covered by the loop. 

### Apply to other datasets

```{r}
# Define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# Loop through each file, apply the filtering function, and save the results
for (file_name in file_list) {
  # Construct the full file path
  file_path <- file.path(directory, file_name)
  
  # Read the data
  df <- data.table::fread(file_path, sep = "\t")
  
  # Apply the filtering function
  filtered_df <- filtering(df)
  
  # Construct the output file name
  output_file_name <- paste0("filtered_", file_name)
  output_file_path <- file.path(directory, output_file_name)
  
  # Save the filtered data
  write.table(filtered_df, file = output_file_path, sep = "\t", row.names = FALSE)
  
  # Print the summary
  print(paste("Summary for:", output_file_name))
  print(dim(filtered_df))
  print(summary(filtered_df$Time))
  print(summary(filtered_df$RR))
  print(table(filtered_df$RR < 300))
  print(table(filtered_df$RR > 1300))
  cat("\n")
}
```

### Check the detail data for Marie-Anne

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("test190031_ADVHRV_2024-05-07.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)
```

```{r}
extract2 <- subset(data_filtered, RR<300)
dim(extract2)
```

```{r}
View(extract2)
```

Finding :

- Among 9 participants, the code is successfully working for 6 participants. Success means that the result of RR is higher than 300 and lower than 1300. 

- There are 3 participants still have the maximum and/or minimum value that is beyond the threshold. Ben and Mary still have observation > 1.3s and Marie-Anne has the observation <0.3s. All observation refers to the last observation that is not covered by the condition of the loop. 

### Next step 

- Create the additional condition of the loop :

1. If the final RR > 1.3 s, delete it --> To handle the issue from Mary and Ben. 

2. If the final RR is within the accepted range of 0.3s - 1.3s, leave it as it is. 

3. If the final RR < 0.3s, do left merge only. If the result of the left merge is <0.3s, do left merge again. If the result of left merge > 1.3 s, remove it. --> To handle the issue from Marie-Anne data. 

### Filtering 4 

I add the conditions above to filter the last row of data.

```{r}
library(dplyr)

filtering <- function(df) {
  
  # Convert RR to seconds
  df <- df %>% mutate(RR = RR / 1000)
  
  # Helper function to compute E_10
  compute_E_10 <- function(df, i) {
    if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
    sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
  }
  
  # Process the data
  # set the starting point
  i <- 1
  while (i < nrow(df)) {
    
    # If RR > 1.3 s, then remove RR_i and its timestamp
    # this is the new condition
    if (!is.na(df$RR[i]) && df$RR[i] > 1.3) {
      df <- df[-i, ]
      next
    }
    
    # if RR < 0.3s, then do all the merging process
    if (!is.na(df$RR[i]) && df$RR[i] < 0.3) {
      if (i + 1 > nrow(df)) break
      RR_r <- df$RR[i] + df$RR[i + 1]
      ER_r <- abs((RR_r - df$RR[i]) / df$RR[i])
      ER_l <- abs((RR_r - df$RR[i - 1]) / df$RR[i - 1])
      E_tot_r <- if (!is.na(ER_r) && !is.na(ER_l)) ER_r + ER_l else NA
      E_10 <- compute_E_10(df, i)
      
      # If RR_r < 1.3s and ER_l <= E_10 and ER_r <= E_10 then 
      # do right merge, replace RR_i+1 by RR_r and delete RR_i and its timestamp
      if (!is.na(RR_r) && !is.na(ER_r) && !is.na(ER_l) && !is.na(E_10) && RR_r < 1.3 && ER_l <= E_10 && ER_r <= E_10) {
        df$RR[i + 1] <- RR_r
        df <- df[-i, ]
        next
      } else {
        if (i - 1 < 1) break
        RR_l <- df$RR[i] + df$RR[i - 1]
        EL_r <- abs((RR_l - df$RR[i]) / df$RR[i])
        EL_l <- abs((RR_l - df$RR[i - 1]) / df$RR[i - 1])
        E_tot_l <- if (!is.na(EL_r) && !is.na(EL_l)) EL_r + EL_l else NA
        E_10_prev <- compute_E_10(df, i - 1)
        
        # If RR_l < 1.3 s and EL_l <= E10 and EL_r <= E_10 then 
        # left merge, replace RR_i by RR_l and delete RR_i-1 and its timestamp
        if (!is.na(RR_l) && !is.na(EL_l) && !is.na(EL_r) && !is.na(E_10_prev) && RR_l < 1.3 && EL_l <= E_10_prev && EL_r <= E_10_prev) {
          df$RR[i - 1] <- RR_l
          df <- df[-i, ]
          next
        } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l > 1.3) {
          if (i + 1 <= nrow(df)) {
            df <- df[-c(i, i + 1), ]
          } else {
            df <- df[-i, ]
          }
          next
        } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r < 1.3 && RR_l > 1.3) {
          df$RR[i + 1] <- RR_r
          df <- df[-i, ]
          next
        } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l < 1.3) {
          df$RR[i - 1] <- RR_l
          df <- df[-i, ]
          next
        } else if (!is.na(RR_l) && !is.na(RR_r) && !is.na(E_tot_r) && !is.na(E_tot_l) && RR_l < 1.3 && RR_r < 1.3 && E_tot_r > 0.4 && E_tot_l > 0.4) {
          if (E_tot_r < E_tot_l) {
            df$RR[i + 1] <- RR_r
            df <- df[-i, ]
          } else {
            df$RR[i - 1] <- RR_l
            df <- df[-i, ]
          }
          next
        }
      }
    }
    # restate the iteration
    i <- i + 1
  }
  
  # Handle the last row
  # if the RR < 0.3
  while (nrow(df) > 0 && df$RR[nrow(df)] < 0.3) {
    
    # compute RR left
    RR_l <- df$RR[nrow(df)] + df$RR[nrow(df)-1]
    
    # if RR_l > 1.3 s
    if (RR_l > 1.3) {
      df <- df[-nrow(df), ] # remove the row
      next
    } else {
      df$RR[nrow(df)-1] <- RR_l # otherwise, do left merge
      df <- df[-nrow(df), ]
    }
  }
  
  # if RR > 1.3
  if (nrow(df) > 0 && df$RR[nrow(df)] > 1.3) {
    df <- df[-nrow(df), ] # remove the row
  }
  
  df <- df %>% mutate(RR = RR * 1000)
  
  return(df)
}
```

Findings:

- The problem of having RR< 0.3 s in the last row is solved. However, there is an equal number showed up as a merging result. 

- I need to check the code for other datasets and check whether the result shows the same thing. 

### Test data to Ben

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("testben1_ADVHRV_2024-02-02_1244.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)

# check the last observation
tail(data_original, 5)
tail(data_filtered, 5)
```
### Test data of Marie-Anne

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("test190031_ADVHRV_2024-05-07.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)

# check the last observation
tail(data_original, 38)
tail(data_filtered, 38)
```
### Test data of Mary

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("testmary1_ADVHRV_2024-01-12.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)

# check the last observation
tail(data_original, 40)
tail(data_filtered, 40)
```
### Test data of Angelica

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("test867601_ADVHRV_2024-05-07.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)

# check the last observation
tail(data_original, 40)
tail(data_filtered, 40)
```
### Test data of NEIL

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("testneil1_ADVHRV_2024-01-15.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)

# check the last observation
tail(data_original, 40)
tail(data_filtered, 40)
```

### Apply to other datasets

```{r}
# Define directory and file list
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"
file_list <- c(
  "testben1_ADVHRV_2024-02-02_1244.txt",
  "testdini1_ADVHRV_20240112.txt", 
  "testamanda1_ADVHRV_2024-01-26.txt",
  "testjenny1_ADVHRV_2024-01-12.txt",
  "testkaris1_ADVHRV_2024-01-26.txt",
  "testmary1_ADVHRV_2024-01-12.txt",
  "testneil1_ADVHRV_2024-01-15.txt",
  "test190031_ADVHRV_2024-05-07.txt", 
  "test867601_ADVHRV_2024-05-07.txt"
)

# Loop through each file, apply the filtering function, and save the results
for (file_name in file_list) {
  # Construct the full file path
  file_path <- file.path(directory, file_name)
  
  # Read the data
  df <- data.table::fread(file_path, sep = "\t")
  
  # Apply the filtering function
  filtered_df <- filtering(df)
  
  # Construct the output file name
  output_file_name <- paste0("filtered_", file_name)
  output_file_path <- file.path(directory, output_file_name)
  
  # Save the filtered data
  write.table(filtered_df, file = output_file_path, sep = "\t", row.names = FALSE)
  
  # Print the summary
  print(paste("Summary for:", output_file_name))
  print(dim(filtered_df))
  print(summary(filtered_df$Time))
  print(summary(filtered_df$RR))
  print(table(filtered_df$RR < 300))
  print(table(filtered_df$RR > 1300))
  cat("\n")
}
```
Findings:

- All data does not have any value < 0.3 s or > 1.3 s anymore. 

### Imputation code

### Prepare the test data

```{r}
directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"

file <- c("testben1_ADVHRV_2024-02-02_1244.txt")

#  file path
full_file_path <- file.path(directory, file)
  
# Read the CSV file
data_original <- data.table::fread(full_file_path, sep = "\t")
  
# change column name
colnames(data_original)[4] <- "conv_type"
```

```{r}
# apply the helper function to one dataset
data_filtered<- filtering(data_original) 
```

```{r}
# check the result
# before filtering
print(dim(data_original))
print(summary(data_original$Time))
print(summary(data_original$RR)) 
table(data_original$RR<300)
table(data_original$RR>1300)

# after filtering
print(dim(data_filtered))
print(summary(data_filtered$Time))
print(summary(data_filtered$RR)) 
table(data_filtered$RR<300)
table(data_filtered$RR>1300)

# check dimension before and after filtering
dim(data_original)-dim(data_filtered)

# check the last observation
head(data_original, 30)
head(data_filtered, 30)
```

### Imputation code

### Version 1 --> condition check within imputation loop

```{r}
library(dplyr)

impute_rr_intervals <- function(df) {
  
  # Convert RR from milliseconds to seconds
  df <- df %>% mutate(RR = RR / 1000)
  df <- df %>% mutate(Time = Time / 1000)
  
  # Helper function to compute E_10
  compute_E_10 <- function(df, i) {
    if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
    sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
  }
  
  # Helper function to compute deviation E_r and E_l
  compute_deviation <- function(RR_j, RR_prev, RR_next) {
    E_l <- abs((RR_j - RR_prev) / RR_prev)
    E_r <- abs((RR_next - RR_j) / RR_j)
    return(list(E_l = E_l, E_r = E_r))
  }
  
  # Iterate over RR intervals
  i <- 1
  while (i < nrow(df)) {
    
    # if the time difference > 1.3 and the time difference is not equal to RR[i+1], execute the imputation
    while (!is.na(df$Time[i]) && !is.na(df$Time[i + 1]) && (df$Time[i + 1] - df$Time[i]) > 1.3 && (df$Time[i + 1] - df$Time[i]) != round(df$RR[i + 1], 0)) {
      
      attempt_count <- 0
      repeat {
        
        # Compute mean and standard deviation
        if (i - 9 <= 0) break
        mu <- mean(df$RR[(i - 9):i])
        sigma <- sd(df$RR[(i - 9):i])
        
        # Create new RR interval
        new_rr <- rnorm(1, mean = mu, sd = sigma)
        
        # Insert the new RR interval into the dataframe
        df <- df %>% add_row(RR = new_rr, .before = i + 1)
        
        # Calculate the new Time value
        T_end <- df$Time[i + 2] 
        new_time <- T_end - df$RR[i + 2] 

        # Insert the new Time value into the dataframe
        df$Time[i + 1] <- new_time
        
        # Check conditions for the new_rr
        deviations <- compute_deviation(new_rr, df$RR[i - 1], df$RR[i + 1])
        
        # Compute E_10 with the new_rr
        E_10 <- compute_E_10(df, i)
        
        # Check if the new RR interval respects the conditions
        if (new_rr > 0.3 && new_rr < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && 
            deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4) {
          break
          
        } else {
          
          # If conditions are not met, remove the last inserted RR interval
          df <- df[-(i + 1), ]
          
          # Adjust E_10 and try again
          E_10 <- E_10 * 1.05
          
          attempt_count <- attempt_count + 1
          if (attempt_count > 3) {
            # If conditions are not met after 3 attempts, break the loop
            break
          }
        }
      }
    }
    i <- i + 1
  }
  
  # Convert RR from seconds back to milliseconds
  df <- df %>% mutate(RR = RR * 1000)
  df <- df %>% mutate(Time = Time * 1000)
  
  # Return the data frame
  return(df)
}

```

Findings :

- The code Version 1 works both in small and large datasets. 

### Version 2 --> condition check within imputation loop

```{r}
library(dplyr)

impute_rr_intervals <- function(df){
  
  # Convert RR and Time from milliseconds to seconds
  df <- df %>% mutate(RR = RR / 1000, Time = Time / 1000)
  
  # Helper function to compute E_10
  compute_E_10 <- function(df, i){
    if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
    sum(abs((df$RR[i:(i+10)] - df$RR[(i-1):(i+9)]) / df$RR[(i-1):(i+9)])) / 10
  }

  # Helper function to compute deviation E_r and E_l
  compute_deviation <- function(RR_j, RR_prev, RR_next) {
    E_l <- abs((RR_j - RR_prev) / RR_prev)
    E_r <- abs((RR_next - RR_j) / RR_j)
    return(list(E_l = E_l, E_r = E_r))
  }
  
  # Iterate over RR intervals
  i <- 1
  iteration_limit <- 10000 # Limit the iteration
  iteration_count <- 0
  
  while (i < nrow(df) && iteration_count < iteration_limit) {
    iteration_count <- iteration_count + 1
    
    # If the time difference >1.3s and the time difference is not equal to RR[i+1], impute
    while (!is.na(df$Time[i]) && !is.na(df$Time[i+1]) && (df$Time[i+1] - df$Time[i]) > 1.3 && (df$Time[i+1] - df$Time[i]) != round(df$RR[i+1], 0)) {
     
      # Compute mean and standard deviation
      if (i - 9 <= 0) break
      mu <- mean(df$RR[(i-9):i])
      sigma <- sd(df$RR[(i-9):i])
      
      # Create new RR interval
      new_rr <- rnorm(1, mean = mu, sd = sigma)
      
      # Insert the new RR interval into the data frame
      df <- df %>% add_row(RR = new_rr, .before = i + 1)
      
      # Calculate the new Time value
      T_end <- df$Time[i+2]
      new_time <- T_end - df$RR[i+2]
      
      # Insert the time value into the data frame
      df$Time[i+1] <- new_time
    }
    
    # Condition check after imputation in each gap
    if (!is.na(df$Time[i]) && !is.na(df$Time[i+1]) && (df$Time[i+1] - df$Time[i]) > 1.3) {
      
      E_10 <- compute_E_10(df, i)
      deviations <- compute_deviation(df$RR[i+1], df$RR[i], df$RR[i+2])
      attempt_count <- 0
      
      # Check the conditions based on table 1
      while (!(df$RR[i+1] > 0.3 && df$RR[i+1] < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4)) {
        
        # Delete last two inserted RRs
        if (nrow(df[i:(i+1)]) > 1) {
          df <- df %>% slice(-c(i, i+1))
        } else {
          df <- df %>% slice(-i)
        }
        
        attempt_count <- attempt_count + 1
        
        if (attempt_count <= 4) {
          
          # Re-impute with a loosened E_10
          E_10 <- E_10 * 1.05
          
          # Re-compute mean and standard deviation
          mu <- mean(df$RR[(i-9):i])
          sigma <- sd(df$RR[(i-9):i])
          
          # Re-compute the new rr
          new_rr <- rnorm(1, mean = mu, sd = sigma)
          
          # Re-compute the new Time 
          T_end <- df$Time[i+2]
          new_time <- T_end - df$RR[i+2]
          df$Time[i+1] <- new_time
          
          # Re-compute the deviation
          deviations <- compute_deviation(new_rr, df$RR[i], df$RR[i+2])
          
        } else {
          
          # Compute last two RR values with deviation
          deviations <- compute_deviation(df$RR[i+1], df$RR[i], df$RR[i+2])
          break
        }
      }
    }
    i <- i + 1
  }
  
  if (iteration_count >= iteration_limit) {
    warning("Reached iteration limit. Possible infinite loop.")
  }
  
  # Convert RR and Time from seconds back to milliseconds
  df <- df %>% mutate(RR = RR * 1000, Time = Time * 1000)
  
  # Return the modified data frame
  return(df)
}

```

Findings:

The code version 2 works in small datasets, but not for the big one. I need more time to test that. 

### Apply the imputation function to the dataset

```{r}
extract4 <- data_original[1:100, ]
```

```{r}
extract4a <- data_filtered[1:500, ]
```

```{r}
data_imputed_4b <- impute_rr_intervals(extract4a)
```

```{r}
summary(extract4a$Time)
summary(extract4a$RR)
dim(extract4a)
head(extract4a)
summary(data_imputed_4b$Time)
summary(data_imputed_4b$RR)
dim(data_imputed_4b)
head(data_imputed_4b)
```

Topics to discuss:

1. We need to discuss how to handle data for the first ten observation --> Now, it still used a BREAK

2. Does the calculation of E_10 should be done before of after the new rows inserted?

### Test to bigger dataset

```{r}
data_imputed <- impute_rr_intervals(data_filtered)
```

```{r}
summary(data_filtered$Time)
summary(data_filtered$RR)
dim(data_filtered)
head(data_filtered)
summary(data_imputed$Time)
summary(data_imputed$RR)
dim(data_imputed)
head(data_imputed)
```
```{r}
extract5 <- subset(data_imputed, RR>1299)
```

Findings:

- The code Version 1 works well for real data, with the total processing time of 30 minutes. 

- The summary statistics shows that the range of RR is between 0.3-1.3. However, there is many datasets with value of 1299 with decimal that is rounded to 1300. I think this is fine. 

### Side note --> This is unused code, but I may need to use that in other time, so I keep it. 

```{r}
# directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"
# 
# file_list <- c("testben1_ADVHRV_2024-02-02_1244.txt")
# 
# # File path
# full_file_path <- file.path(directory, file_list)
#   
# # Read the CSV file
# data <- fread(full_file_path, sep = "\t")
#   
# # Change column name
# colnames(data)[4] <- "category"
```

```{r}
# data$RR_second <- data$RR/1000
```

```{r}
# data$dummy_thres <- ifelse(data$RR_second < 0.3 | data$RR_second > 1.3, 1, 0)
```

```{r}
# data$dummy_device <- ifelse(data$category=="U" | data$category=="|", 1, 0)
```

```{r}
# data$dummy_both <- ifelse(data$dummy_device==1 & data$dummy_thres==1, 1, 0)
```

```{r}
# data %>%
#   summarise(count_paper = sum(dummy_thres==1),
#             count_device = sum(dummy_device==1), 
#             count_overlap = sum(dummy_both==1),
#             percent_paper = round((mean(dummy_thres) * 100),2),
#             percent_device = round((mean(dummy_device) * 100),2), 
#             percent_overlap = round((mean(dummy_both) * 100),2)
#             )
```

### Side note 2

### Follow up: Compute correlation of time-of-day and artifacts from paper. (Individual Code)

```{r}
# directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"
# 
# file_list <- c("test867601_ADVHRV_2024-05-07.txt")
# 
# # File path
# full_file_path <- file.path(directory, file_list)
#   
# # Read the CSV file
# data <- fread(full_file_path, sep = "\t")
#   
# # Change column name
# colnames(data)[4] <- "category"
# 
# # library
# library(data.table)
# library(lubridate)
# 
# # define initial time
# initial_time <- "15:58:52"
# 
# # define the start time
# start_time <- as.POSIXct(initial_time, format = "%H:%M:%S")
# 
# # convert cumulative time and RR in milliseconds to seconds
# data$time_seconds <- as.numeric(data$Time) / 1000
# 
# # convert RR into seconds
# data$RR_second = data$RR / 1000
# 
# # calculate the clock time
# data$clock_time <- start_time + data$time_seconds
# 
# # Extract the time part (HH:MM:SS) from Clock_time
# data$time_only <- format(data$clock_time, format = "%H:%M:%S")
# 
# # categorize the time_only
# data$cat_time <- fifelse(data$time_only >= "00:00:00" & data$time_only < "04:00:00", 1,
#                    fifelse(data$time_only >= "04:00:00" & data$time_only < "08:00:00", 2,
#                    fifelse(data$time_only >= "08:00:00" & data$time_only < "12:00:00", 3,
#                    fifelse(data$time_only >= "12:00:00" & data$time_only < "16:00:00", 4,
#                    fifelse(data$time_only >= "16:00:00" & data$time_only < "20:00:00", 5,
#                    fifelse(data$time_only >= "20:00:00" & data$time_only < "24:00:00", 6, NA))))))
# 
# # categorize the artifacts based on threshold < 0.3 s and > 1.3 s
# data$dummy_thres <- ifelse(data$RR_second < 0.3 | data$RR_second > 1.3, 1, 0) 
# 
# # Display the updated data
# head(data, 100)
# 
# # summary of artifacts for each time window
# result <- data %>%
#   group_by(cat_time) %>%
#   summarise(count_artifacts = sum(dummy_thres), 
#             percent_artifacts = round((count_artifacts / sum(data$dummy_thres) * 100),2)
#             )
# 
# print(result)
```
### Archive

### Create helper function

```{r}
# library(data.table)
# library(lubridate)
# library(dplyr)
# 
# # Define the helper function
# process_data <- function(data, initial_time) {
#   # Convert the initial time to a POSIXct object
#   start_time <- ymd_hms(initial_time)
#   
#   # Convert cumulative time and RR in milliseconds to seconds
#   data[, time_seconds := Time / 1000]
#   data[, RR_second := RR / 1000]
#   
#   # Calculate the clock time
#   data[, clock_time := start_time + seconds(time_seconds)]
#   
#   # Extract the time part (HH:MM:SS) from clock_time
#   data[, time_only := format(clock_time, format = "%H:%M:%S")]
#   
#   # Categorize the time_only
#   data[, cat_time := fifelse(time_only >= "00:00:00" & time_only < "04:00:00", 1,
#                       fifelse(time_only >= "04:00:00" & time_only < "08:00:00", 2,
#                       fifelse(time_only >= "08:00:00" & time_only < "12:00:00", 3,
#                       fifelse(time_only >= "12:00:00" & time_only < "16:00:00", 4,
#                       fifelse(time_only >= "16:00:00" & time_only < "20:00:00", 5,
#                       fifelse(time_only >= "20:00:00" & time_only < "24:00:00", 6, NA))))))]
#   
#   # Categorize the artifacts based on threshold < 0.3 s and > 1.3 s
#   data[, dummy_thres := ifelse(RR_second < 0.3 | RR_second > 1.3, 1, 0)]
#   
#   return(data)
# }
# ```
# 
# 
# ```{r}
# library(data.table)
# library(lubridate)
# library(dplyr)
# 
# # Define the helper function
# process_data <- function(data, initial_time) {
#   
#   # Convert the initial time to a POSIXct object
#   start_time <- ymd_hms(initial_time)
#   
#   # Convert cumulative time and RR in milliseconds to seconds
#   data[, time_seconds := Time / 1000]
#   data[, RR_second := RR / 1000]
#   
#   # Calculate the clock time
#   data$clock_time <- start_time + data$time_seconds
#   
#   # Extract the time part (HH:MM:SS) from clock_time
#   data[, time_only := format(clock_time, format = "%H:%M:%S")]
#   
#   # Categorize the time_only
#   data[, cat_time := fifelse(time_only >= "00:00:00" & time_only < "04:00:00", 1,
#                       fifelse(time_only >= "04:00:00" & time_only < "08:00:00", 2,
#                       fifelse(time_only >= "08:00:00" & time_only < "12:00:00", 3,
#                       fifelse(time_only >= "12:00:00" & time_only < "16:00:00", 4,
#                       fifelse(time_only >= "16:00:00" & time_only < "20:00:00", 5,
#                       fifelse(time_only >= "20:00:00" & time_only < "24:00:00", 6, NA))))))]
#   
#   # Categorize the artifacts based on threshold < 0.3 s and > 1.3 s
#   data[, dummy_thres := ifelse(RR_second < 0.3 | RR_second > 1.3, 1, 0)]
#   
#   return(data)
# }
```


```{r}
# library(data.table)
# 
# directory <- "/Users/auliadinirafsanjani/Dropbox (University of Michigan)/WorkLife_ECGTest/Test"
# file_list <- c("testben1_ADVHRV_2024-02-02_1244.txt")
# 
# # set result as list
# start_times_list <- list()
# 
# for (i in seq_along(file_list)) {
#   
#     # input the file list
#     file <- file_list[i]
#     
#     # file path
#     full_file_path <- file.path(directory, file)
# 
#     # Read the first line to get the recording start time
#     first_line <- tryCatch({
#       readLines(full_file_path, n = 1)
#     }, error = function(e) {
#       NA
#     })
# 
#     if (!is.na(first_line)) {
#       
#       # extract the recording start time 
#       time_pattern <- "Recording start time:\\s*(\\d{1,2}:\\d{2}:\\d{2}\\s*[AP]M)"
#       start_time <- sub(time_pattern, "\\1", first_line)
# 
#       # reformat the start time into 24 hr
#       start_time_24h <- format(strptime(start_time, "%I:%M:%S %p"), "%H:%M:%S")
#       
#       # append the start time to the list
#       start_times_list[[i]] <- start_time_24h
#       
#     } else {
#       
#       # Handle case where first line could not be read
#       start_times_list[[i]] <- NA
#     }
# }
# 
# # Print the list of start times
# print(start_times_list)
```


### Filtering 1

```{r}
# library(dplyr)
# 
# filtering <- function(df) {
# 
# # convert RR to seconds
# df <- df %>%
#       mutate (RR=RR/1000)
# 
# # helper function to compute E_r
# compute_E_r <- function (df,j){
#   if (j+1 > nrow(df)) return (NA)
#   abs((df$RR[j+1]-df$RR[j]) / df$RR[j]) # equation 6
# }
# 
# # helper function to compute E_l
# compute_E_l <- function (df,j) {
#   if (j-1 < 1) return (NA)
#   abs((df$RR[j]-df$RR[j-1]) / df$RR[j-1]) # equation 7
# } 
# 
# # helper function to compute E_10
# compute_E_10 <- function(df,i) {
#   if(i+10 > nrow(df) || i-1 < 1) return (NA)
#   sum(abs((df$RR[i:(i+10)] - df$RR[(i-1):(i+9)]) / df$RR[(i-1):(i+9)])) /10
# } 
# 
# # process the data
# # for RRi < 0.3s compute RR_R, E_r, E_l, E_totr, E_10
# for (i in 2:(nrow(df)-1)){
#   
#   if (!is.na(df$RR[i]) && df$RR[i] < 0.3){
#     if (i+1 > nrow(df)) break
#       RR_r <- df$RR[i] + df$RR[i+1]
#       E_r <- compute_E_r(df,i)
#       E_l <- compute_E_l(df,i)
#       E_tot_r <- if (!is.na(E_r) && !is.na(E_l)) E_r + E_l else NA
#       E_10 <- compute_E_10(df,i)
#    
#     #  if RR_r < 1.3s and ER_l <= E_10 and ER_r <= E_10 then 
#       # do right merge, replace RR_i+1 by RR_r and delete RR_i and its timestamp
#     if (!is.na(RR_r) && !is.na(E_l) && !is.na(E_r) && !is.na(E_10) && RR_r < 1.3 && E_l <= E_10 && E_r <= E_10){
#       df$RR[i+1] <- RR_r
#       df <- df[-i,]
#       
#       # else compute RR_l = RR_i + RR_i-1 and EL_r (equation 6), EL_l (equation 7) and Etot_l (equation 5)
#     } else { 
#       if (i-1 < 1) break
#       RR_l <- df$RR[i] + df$RR[i-1]
#       EL_r <- compute_E_r(df, i-1)
#       EL_l <- compute_E_l(df, i-1)
#       E_tot_l <- if (!is.na(EL_r) && !is.na(EL_l)) EL_r + EL_l else NA
#       E_10_prev <- compute_E_10(df, i-1)
#       
#       # if RR_l < 1.3 s and EL_l <= E10 and EL_r <= E_10 then 
#       # left merge, replace R_i by RR_l and delete RR_i-1 and its timestamp
#     if (!is.na(RR_l) && !is.na(EL_l) && !is.na(EL_r) && !is.na(E_10_prev) && RR_l < 1.3 && EL_l <= E_10_prev && EL_r <= E_10_prev) {
#       df$RR[i-1] <- RR_l
#       df <- df[-i,]
#       
#       # else if RR_r > 1.3s and RR_l > 1.3 s then
#       # delete both RR_i and RR_i+1
#     } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l > 1.3) {
#       if (i+1 <= nrow(df)) {
#         df <- df[-c(i,i+1),]
#       } else {
#         df <- df[-i,] }
#       
#       # else if RR_r < 1.3 s and RR_l > 1.3 s then 
#       # Replace RR_i+1 and RR_r and delete RR_i and its timestamp
#       } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r < 1.3 && RR_l > 1.3){
#         df$RR[i+1] <- RR_r
#         df <- df[-i,]
#         
#         # else if RR_r > 1.3 s and RR_l < 1.3 s then 
#         # replace RR_i-1 by RR_l and delete RR_i and its timestamp
#       } else if (!is.na(RR_r) && !is.na(RR_l) && RR_r > 1.3 && RR_l < 1.3){
#        df$RR[i-1] <- RR_l
#        df <- df[-i,]
#        
#        # else if both RR_l and RR_r < 1.3 s but errors Etot_r and Etot_l are higher than 0.4 then 
#        # keep the one with smaller error
#       } else if (!is.na(RR_l) && !is.na(RR_r) && !is.na(E_tot_r) && !is.na(E_tot_l) && RR_l < 1.3 && RR_r < 1.3 && E_tot_r > 0.4 && E_tot_l > 0.4){
#         if (E_tot_r < E_tot_l){ # if right < left, use right 
#          df$RR[i+1] <- RR_r
#          df <- df[-i,]
#         } else { # otherwise, use left
#           df$RR[i-1] <- RR_l
#           df <- df[-i,]
#         }
#       }
#     }
#   }
# }
# 
# df <- df %>% mutate(RR = RR * 1000)
# 
# return(df)
# }
```


```{r}
# ext9 <- subset(data_filtered, RR < 300)
```

```{r}
# print(ext9)
```

```{r}
# Apply the helper function to the sample data
# new_data <- process_rr_intervals(ext8)
```

```{r}
# library(openxlsx)
# write.xlsx(data_original, file = "data.xlsx")
```

```{r}
# head(data_clean,100)
```


```{r}
# library(ggplot2)
# library(gridExtra)
# 
# df <- result_df1
# 
# # Assuming your data frame is named `df`
# # Get the unique dataset names
# dataset_names <- unique(df$data_name)
# 
# # Create a list to store the plots
# plot_list <- list()
# 
# # Loop through each dataset name and create a plot
# for (dataset in dataset_names) {
#   # Subset the data for the current dataset
#   subset_df <- df[df$data_name == dataset, ]
#   
#   # Create the plot
#   p <- ggplot(subset_df, aes(x = cat_date_time, y = count_artifacts, fill = data_name)) +
#     geom_col() +
#     labs(title = paste("Count of Artifacts:", dataset),
#          x = "Categorized Time-of-Day",
#          y = "Count of Artifacts") +
#     theme_minimal() +
#     theme(legend.position = "none",
#           axis.text.x = element_text(angle = 45, hjust = 1))
#   
#   # Add the plot to the list
#   plot_list[[dataset]] <- p
# }
# 
# plot_list
```

```{r}
# library(dplyr)
# 
# impute_rr_intervals <- function(df) {
#   
#   # Convert RR from milliseconds to seconds
#   df <- df %>% mutate(RR = RR / 1000)
#   df <- df %>% mutate(Time = Time / 1000)
#   
#   # Initialize 'No' column with sequential values
#   #df$No <- seq_len(nrow(df))
#   
#   # Helper function to compute E_10
#   compute_E_10 <- function(df, i) {
#     if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
#     sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
#   }
#   
#   # Helper function to compute deviation E_r and E_l
#   compute_deviation <- function(RR_j, RR_prev, RR_next) {
#     E_l <- abs((RR_j - RR_prev) / RR_prev)
#     E_r <- abs((RR_next - RR_j) / RR_j)
#     return(list(E_l = E_l, E_r = E_r))
#   }
#   
#   # Iterate over RR intervals
#   for (i in 1:(nrow(df) - 1)) {
#     while (!is.na(df$Time[i]) && !is.na(df$Time[i + 1]) && (df$Time[i + 1] - df$Time[i]) > 1.3) { # Time or RR?
#       
#       attempt_count <- 0
#       repeat {
#         
#         # Compute mean and standard deviation
#         if (i- 9 <= 0) break
#         mu <- mean(df$RR[(i - 9):i])
#         sigma <- sd(df$RR[(i - 9):i])
#         
#         # Create new RR interval
#         new_rr <- rnorm(1, mean = mu, sd = sigma)
#         
#         # Increment 'No' values for rows below insertion point
#         #df <- df %>% mutate(No = ifelse(No > i, No + 1, No))
#         
#         # Insert the new RR interval into the dataframe
#         df <- df %>% add_row(RR = new_rr, .before = i + 1)
#         #No = i + 1, 
#         
#         # Calculate the new Time value
#         T_end <- df$Time[i + 2] 
#         new_time <- T_end - df$RR[i + 1]
# 
#         # Insert the new Time value into the dataframe
#         df$Time[i + 1] <- new_time
#         
#         # Check conditions for the new_rr
#         deviations <- compute_deviation(new_rr, df$RR[i - 1], df$RR[i + 1])
#         
#         # Compute E_10 with the new_rr
#         E_10 <- compute_E_10(df, i)
#         
#         # Check if the new RR interval respects the conditions
#         if (new_rr > 0.3 && new_rr < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && 
#             deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4) {
#           break
#         } else {
#           # If conditions are not met, remove the last inserted RR interval
#           df <- df[-(i + 1), ]
#           
#           # Adjust E_10 and try again
#           E_10 <- E_10 * 1.05
#           
#           attempt_count <- attempt_count + 1
#           if (attempt_count > 3) {
#             # If conditions are not met after 3 attempts, break the loop
#             break
#           }
#         }
#       }
#     }
#   }
#   
#   # Reset 'No' column to sequential values
#   #df$No <- seq_len(nrow(df))
#   
#   # Convert RR from seconds back to milliseconds
#   df <- df %>% mutate(RR = RR * 1000)
#   df <- df %>% mutate(Time = Time * 1000)
#   
#   # Return the dataframe
#   return(df)
# }
```

### Version 1

```{r}
# library(dplyr)
# 
# impute_rr_intervals <- function(df) {
#   
#   # Convert RR from milliseconds to seconds
#   df <- df %>% mutate(RR = RR / 1000)
#   df <- df %>% mutate(Time = Time / 1000)
#   
#   # Helper function to compute E_10
#   compute_E_10 <- function(df, i) {
#     if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
#     sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
#   }
#   
#   # Helper function to compute deviation E_r and E_l
#   compute_deviation <- function(RR_j, RR_prev, RR_next) {
#     E_l <- abs((RR_j - RR_prev) / RR_prev)
#     E_r <- abs((RR_next - RR_j) / RR_j)
#     return(list(E_l = E_l, E_r = E_r))
#   }
#   
#   # Iterate over RR intervals
#   for (i in 1:(nrow(df) - 1)) {
#     while (!is.na(df$Time[i]) && !is.na(df$Time[i + 1]) && (df$Time[i + 1] - df$Time[i]) > 1.3) {
#       
#       attempt_count <- 0
#       repeat {
#         
#         # Compute mean and standard deviation
#         if (i- 9 <= 0) break
#         mu <- mean(df$RR[(i - 9):i])
#         sigma <- sd(df$RR[(i - 9):i])
#         
#         # Create new RR interval
#         new_rr <- rnorm(1, mean = mu, sd = sigma)
#         
#         # Insert the new RR interval into the dataframe
#         df <- df %>% add_row(RR = new_rr, .before = i + 1)
#         
#         # Calculate the new Time value
#         T_end <- df$Time[i + 2] 
#         new_time <- T_end - df$RR[i + 1]
# 
#         # Insert the new Time value into the dataframe
#         df$Time[i + 1] <- new_time
#         
#         # Check conditions for the new_rr
#         deviations <- compute_deviation(new_rr, df$RR[i - 1], df$RR[i + 1])
#         
#         # Compute E_10 with the new_rr
#         E_10 <- compute_E_10(df, i)
#         
#         # Check if the new RR interval respects the conditions
#         if (new_rr > 0.3 && new_rr < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && 
#             deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4) {
#           break
#         } else {
#           # If conditions are not met, remove the last inserted RR interval
#           df <- df[-(i + 1), ]
#           
#           # Adjust E_10 and try again
#           E_10 <- E_10 * 1.05
#           
#           attempt_count <- attempt_count + 1
#           if (attempt_count > 3) {
#             # If conditions are not met after 3 attempts, break the loop
#             break
#           }
#         }
#       }
#     }
#   }
#   
#   # Convert RR from seconds back to milliseconds
#   df <- df %>% mutate(RR = RR * 1000)
#   df <- df %>% mutate(Time = Time * 1000)
#   
#   # Return the dataframe
#   return(df)
# }
```


### Version 2


```{r}
# library(dplyr)
# 
# impute_rr_intervals <- function(df) {
#   
#   # Convert RR from milliseconds to seconds
#   df <- df %>% mutate(RR = RR / 1000)
#   df <- df %>% mutate(Time = Time / 1000)
#   
#   # Helper function to compute E_10
#   compute_E_10 <- function(df, i) {
#     if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
#     sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
#   }
#   
#   # Helper function to compute deviation E_r and E_l
#   compute_deviation <- function(RR_j, RR_prev, RR_next) {
#     E_l <- abs((RR_j - RR_prev) / RR_prev)
#     E_r <- abs((RR_next - RR_j) / RR_j)
#     return(list(E_l = E_l, E_r = E_r))
#   }
#   
#   # Iterate over RR intervals
#   i <- 1
#   while (i < nrow(df)) {
#     while (!is.na(df$Time[i]) && !is.na(df$Time[i + 1]) && (df$Time[i + 1] - df$Time[i]) > 1.3) {
#       
#       attempt_count <- 0
#       repeat {
#         
#         # Compute mean and standard deviation
#         if (i - 9 <= 0) break
#         mu <- mean(df$RR[(i - 9):i])
#         sigma <- sd(df$RR[(i - 9):i])
#         
#         # Create new RR interval
#         new_rr <- rnorm(1, mean = mu, sd = sigma)
#         
#         # Insert the new RR interval into the dataframe
#         df <- df %>% add_row(RR = new_rr, .before = i + 1)
#         
#         # Calculate the new Time value
#         T_end <- df$Time[i + 2] 
#         new_time <- T_end - df$RR[i + 1]
# 
#         # Insert the new Time value into the dataframe
#         df$Time[i + 1] <- new_time
#         
#         # Check conditions for the new_rr
#         deviations <- compute_deviation(new_rr, df$RR[i - 1], df$RR[i + 1])
#         
#         # Compute E_10 with the new_rr
#         E_10 <- compute_E_10(df, i)
#         
#         # Check if the new RR interval respects the conditions
#         if (new_rr > 0.3 && new_rr < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && 
#             deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4) {
#           break
#         } else {
#           # If conditions are not met, remove the last inserted RR interval
#           df <- df[-(i + 1), ]
#           
#           # Adjust E_10 and try again
#           E_10 <- E_10 * 1.05
#           
#           attempt_count <- attempt_count + 1
#           if (attempt_count > 3) {
#             # If conditions are not met after 3 attempts, break the loop
#             break
#           }
#         }
#       }
#     }
#     i <- i + 1
#   }
#   
#   # Convert RR from seconds back to milliseconds
#   df <- df %>% mutate(RR = RR * 1000)
#   df <- df %>% mutate(Time = Time * 1000)
#   
#   # Return the dataframe
#   return(df)
# }
# 
```
# 

### Version 3

```{r}
# library(dplyr)
# 
# impute_rr_intervals <- function(df) {
#   
#   # Convert RR from milliseconds to seconds
#   df <- df %>% mutate(RR = RR / 1000)
#   df <- df %>% mutate(Time = Time / 1000)
#   
#   # Helper function to compute E_10
#   compute_E_10 <- function(df, i) {
#     if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
#     sum(abs((df$RR[i:(i + 10)] - df$RR[(i - 1):(i + 9)]) / df$RR[(i - 1):(i + 9)])) / 10
#   }
#   
#   # Helper function to compute deviation E_r and E_l
#   compute_deviation <- function(RR_j, RR_prev, RR_next) {
#     E_l <- abs((RR_j - RR_prev) / RR_prev)
#     E_r <- abs((RR_next - RR_j) / RR_j)
#     return(list(E_l = E_l, E_r = E_r))
#   }
#   
#   # Iterate over RR intervals
#   i <- 1
#   while (i < nrow(df)) {
#     while (!is.na(df$Time[i]) && !is.na(df$Time[i + 1]) && (df$Time[i + 1] - df$Time[i]) > 1.3) {
#       
#       attempt_count <- 0
#       repeat {
#         
#         # Compute mean and standard deviation
#         if (i - 9 <= 0) break
#         mu <- mean(df$RR[(i - 9):i])
#         sigma <- sd(df$RR[(i - 9):i])
#         
#         # Create new RR interval
#         new_rr <- rnorm(1, mean = mu, sd = sigma)
#         
#         # Insert the new RR interval into the dataframe
#         df <- df %>% add_row(RR = new_rr, .before = i + 1)
#         
#         # Calculate the new Time value
#         T_end <- df$Time[i + 2] 
#         new_time <- T_end - df$RR[i + 1]
# 
#         # Insert the new Time value into the dataframe
#         df$Time[i + 1] <- new_time
#         
#         # Check conditions for the new_rr
#         deviations <- compute_deviation(new_rr, df$RR[i - 1], df$RR[i + 1])
#         
#         # Compute E_10 with the new_rr
#         E_10 <- compute_E_10(df, i)
#         
#         # Check if the new RR interval respects the conditions
#         if (new_rr > 0.3 && new_rr < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && 
#             deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4) {
#           break
#         } else {
#           # If conditions are not met, remove the last inserted RR interval
#           df <- df[-(i + 1), ]
#           
#           # Adjust E_10 and try again
#           E_10 <- E_10 * 1.05
#           
#           attempt_count <- attempt_count + 1
#           if (attempt_count > 3) {
#             # If conditions are not met after 3 attempts, break the loop
#             break
#           }
#         }
#       }
#     }
#     i <- i + 1
#   }
#   
#   # Convert RR from seconds back to milliseconds
#   df <- df %>% mutate(RR = RR * 1000)
#   df <- df %>% mutate(Time = Time * 1000)
#   
#   # Return the dataframe
#   return(df)
# }

```


### Version 2

```{r}
# library(dplyr)
# 
# impute_rr_intervals <- function(df){
#   
#   # Set the seed
#   set.seed(999)
#   
#   # Convert RR and Time from milliseconds to seconds
#   df <- df %>% mutate(RR = RR / 1000, Time = Time / 1000)
#   
#   # Helper function to compute E_10
#   compute_E_10 <- function(df, i){
#     if (i + 10 > nrow(df) || i - 1 < 1) return(NA)
#     sum(abs((df$RR[i:(i+10)] - df$RR[(i-1):(i+9)]) / df$RR[(i-1):(i+9)])) / 10
#   }
# 
#   # Helper function to compute deviation E_r and E_l
#   compute_deviation <- function(RR_j, RR_prev, RR_next) {
#     E_l <- abs((RR_j - RR_prev) / RR_prev)
#     E_r <- abs((RR_next - RR_j) / RR_j)
#     return(list(E_l = E_l, E_r = E_r))
#   }
#   
#   # Iterate over RR intervals
#   i <- 1
#   while (i < nrow(df)) {
#     
#     # If the time difference >1.3s and the time difference is not equal to RR[i+1], impute
#     while (!is.na(df$Time[i]) && !is.na(df$Time[i+1]) && (df$Time[i+1] - df$Time[i]) > 1.3 && (df$Time[i+1] - df$Time[i]) != round(df$RR[i+1], 0)) {
#      
#       # Compute mean and standard deviation
#       if (i - 9 <= 0) break
#       mu <- mean(df$RR[(i-9):i])
#       sigma <- sd(df$RR[(i-9):i])
#       
#       # Create new RR interval
#       new_rr <- rnorm(1, mean = mu, sd = sigma)
#       
#       # Insert the new RR interval into the data frame
#       df <- df %>% add_row(RR = new_rr, .before = i + 1)
#       
#       # Calculate the new Time value
#       T_end <- df$Time[i+2]
#       new_time <- T_end - df$RR[i+2]
#       
#       # Insert the time value into the data frame
#       df$Time[i+1] <- new_time
#     }
#     
#     # Condition check after imputation in each gap
#     if (!is.na(df$Time[i]) && !is.na(df$Time[i+1]) && (df$Time[i+1] - df$Time[i]) > 1.3) {
#       
#       E_10 <- compute_E_10(df, i)
#       deviations <- compute_deviation(df$RR[i+1], df$RR[i], df$RR[i+2])
#       attempt_count <- 0
#       
#       # Check the conditions based on table 1
#       while (!(df$RR[i+1] > 0.3 && df$RR[i+1] < 1.3 && !is.na(E_10) && deviations$E_r <= E_10 && deviations$E_r <= 0.4 && deviations$E_l <= E_10 && deviations$E_l <= 0.4)) {
#         
#         # Delete last two inserted RRs
#         if (nrow(df[i:(i+1)]) > 1) {
#           df <- df %>% slice(-c(i, i+1))
#         } else {
#           df <- df %>% slice(-i)
#         }
#         
#         attempt_count <- attempt_count + 1
#         
#         if (attempt_count <= 4) {
#           
#           # Re-impute with a loosened E_10
#           E_10 <- E_10 * 1.05
#           
#           # Re-compute mean and standard deviation
#           mu <- mean(df$RR[(i-9):i])
#           sigma <- sd(df$RR[(i-9):i])
#           
#           # Re-compute the new rr
#           new_rr <- rnorm(1, mean = mu, sd = sigma)
#           
#           # Re-compute the new Time 
#           T_end <- df$Time[i+2]
#           new_time <- T_end - df$RR[i+2]
#           df$Time[i+1] <- new_time
#           
#           # Re-compute the deviation
#           deviations <- compute_deviation(new_rr, df$RR[i], df$RR[i+2])
#           
#         } else {
#           
#           # Compute last two RR values with deviation
#           deviations <- compute_deviation(df$RR[i+1], df$RR[i], df$RR[i+2])
#           break
#         }
#       }
#     }
#     i <- i + 1
#   }
#   
#   # Convert RR and Time from seconds back to milliseconds
#   df <- df %>% mutate(RR = RR * 1000, Time = Time * 1000)
#   
#   # Return the modified data frame
#   return(df)
# }
```
